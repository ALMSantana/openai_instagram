Hipsters.tech, o podcast de tecnologia e outras modinhas. Olá caríssimo ouvinte, seja muito bem-vindo a mais um episódio do seu podcast favorito. Meu nome é Paulo Silveira, esse aqui é o Hipsters.tech e temos um episódio sobre NVIDIA, Inteligência Artificial e Comunidade. E é um episódio muito especial porque eu vou trazer dois convidados que são palestrantes do Dev Leaders 2023, que é uma conferência para pessoas que estão buscando ou estão em liderança, em gestão de tecnologia. Então vamos lá para o podcast ver com quem que a gente vai conversar. Para essa conversa de hoje eu estou aqui com o Jomar Silva, ele é Developer Relations Manager da América Latina para a NVIDIA, a empresa aí que está mais do que na moda e também vai ser um dos palestrantes no Dev Leaders Conference, que a gente vai deixar o link aqui no podcast para que você se inscreva. Tudo bom com você Jomar? Tudo jóia, obrigado pela oportunidade. E além do Jomar, eu estou aqui com o CTO da Lura, o Sérgio Lopes, que frequentemente está no Fora de Controle, o podcast só de IA e que também é palestrante do Dev Leaders. Oi Sérgio, tudo bom com você? Oi Paulo, oi Jomar, tudo bom pessoal? Para a gente começar essa conversa, eu quero pegar isso que está em alta aí com o Jomar, que é a NVIDIA, as GPUs, ou a falta das GPUs, correto? Eu queria entender, Jomar, como que é hoje que essa busca está grande, os GPUs estavam com os preços lá no teto já desde a pandemia e agora me parece que com esse monte de Big Techs correndo atrás para criar sua própria inteligência artificial, seus próprios modelos em larga escala, linguísticos, o que me falaram é que hoje em dia tem as empresas aí que criam data center a cada três dias e só não fazem mais porque a NVIDIA não está fabricando o suficiente, então como que é, o que está acontecendo nesse mercado? É realmente para a inteligência artificial esse maior uso das vendas hoje em dia? Isso é, a próxima vez que eu comprar uma NVIDIA, na placa vai vir escrito assim, você pode jogar o Diablo 4 em resolução 4K, vai vir escrito você pode treinar a sua assistente pessoal em 4K, tem essa mudança mesmo? É óbvio, estou fazendo uma brincadeira porque esse mecanismo é muito mais para as empresas, mas como que a gente enxerga hoje em dia a NVIDIA, GPU, essa escassez, o que está acontecendo? Na realidade são duas linhas de GPUs, todo o trabalho que eu faço é com a divisão enterprise da NVIDIA, então as GPUs enterprise não são as GPUs que a galera tem em casa para jogar Diablo, são GPUs para data center, profissionais, são servidores com múltiplas GPUs e no caso de aplicações como o chat GPT, a gente está falando aí de centenas de servidores, cada servidor com múltiplas GPUs dentro, a capacidade computacional é muito grande e não são as GPUs gamer que são utilizadas para as aplicações de inteligência artificial, as GPUs gamer, isso vale a pena explicar, são GPUs que são desenvolvidas e fabricadas por parceiros da NVIDIA, e as GPUs de data center são GPUs específicas para outros workloads, então a GPU gamer, ela foi pensada para trabalhar processando o jogo, enquanto uma GPU, por exemplo, para treinamento de rede neural, ela foi toda desenvolvida e produzida o hardware para esse tipo de trabalho específico, uma GPU que pode ficar ligada trabalhando 24 horas por dia durante semanas, que é isso que você leva para treinar alguns modelos específicos de rede neural, então as GPUs estão por trás de toda essa revolução, mas mais do que as GPUs, o que possibilitou tudo isso é uma grande evolução dentro do mundo de software, hoje a NVIDIA tem mais engenheiro de software do que de hardware, e nós temos um stack com mais de 500 bibliotecas e mais de 150 SDKs, então isso foi construído ao longo das últimas décadas, conversando com cada empresa que tinha uma necessidade de fazer processamento acelerado, isso fez com que ao longo dos anos a NVIDIA tenha desenvolvido frameworks para suportar diversas aplicações, então muita gente falando do chat GPT, achando que a IA generativa chegou no seu auge, é a ponta do iceberg do que está vindo pela frente, para dar um exemplo disso que eu falo de várias verticais, nós temos hoje uma área específica de IA generativa na NVIDIA chamada Bionimo, que é a utilização de IA generativa para pesquisa de novas proteínas e novos remédios, medicamentos, isso está fazendo com que o tempo de desenvolvimento de um fármaco novo ou de uma vacina se reduza de anos para meses em alguns casos, então as aplicações de IA generativa, as aplicações de inteligência artificial, elas hoje permeiam toda e qualquer vertical de mercado, eu sou responsável entre outras coisas da NVIDIA pelo nosso programa de startups, Inception, a gente já tem mais de 500 startups na América Latina e é uma mudança muito engraçada, porque há uns anos atrás a gente conhecia startups que diziam, eu sou uma startup de inteligência artificial, isso raramente acontece hoje, são startups de agricultura, startups para retail, startups para industrial, todo mundo usa inteligência artificial, mas as startups já não se identificam como startups de IA, então a permeabilidade da inteligência artificial, a facilidade no uso de inteligência artificial e principalmente hoje a facilidade do uso das GPUs da NVIDIA, que foi resultado de anos e anos e anos de investimento, é o que permite a gente ter todo esse boom de novas aplicações e de inovações e o nosso modelo de ir ao mercado, ele é muito interessante, porque nós fazemos isso através de um ecossistema de parceiros que vão desde os grandes ISVs, que todo mundo conhece, até pequenas empresas e pequenas startups do mundo todo, já são mais de 14 mil startups fazendo parte do Inception em mais de 100 países, e o que a gente fez nos últimos anos do ponto de vista do software foi isso, foi facilitar e simplificar cada vez mais o uso da GPU, para que mais pessoas, mais empresas pudessem utilizar essas tecnologias de aceleração, um exemplo disso é há 10 anos atrás, você precisava saber C e CUDA para poder programar uma GPU, hoje qualquer aluno de Python, que saiba o básico do Python, já consegue botar uma GPU para trabalhar bonito, como eu gosto de brincar, a GPU só fica feliz quando está quentinha, então hoje qualquer pessoa que trabalha em Python, aprender o básico de Python, para dar um exemplo concreto, aprender a usar pandas, NumPy, Scikit-learn, já consegue fazer tudo isso rodar em GPU, sem ter que estudar um monte de coisas sobre o paralelismo, os algoritmos distribuídos e coisas com esse grau de complexidade. Jomar, você usou um termo aí, ASVs, eu não peguei direito o que é exatamente? A ASVs são as grandes empresas de software, né? Ah, entendi, seriam as big techs que eu citei. São empresas como Adobe, como Autodesk, e por aí vai, são empresas que fabricam esses softwares em grande escala, uma coisa interessante que acontece, é que muitas pessoas já usam muita tecnologia de Adobe NVIDIA há alguns anos, só nunca enxergaram que era a NVIDIA que estava por trás, então existem diversas aplicações já no mercado que utilizam essas acelerações. Quando a gente fala de cloud computing, então, é super comum isso acontecer, mas a gente está vendo agora a IA generativa chegar na mão do usuário final, então, por exemplo, Adobe tem alguns recursos de software hoje com IA generativa para fazer edição de imagens que são realmente impressionantes, é coisa que era ficção científica talvez há cinco anos atrás, então a ideia é que essas tecnologias, através desse ecossistema de parceiros, startups, de ASVs, ela está chegando cada vez mais na mão das pessoas. E o chat GPT se popularizou muito rápido por conta disso, é a primeira vez que a população em geral tem acesso a uma inteligência artificial que é simples de utilizar, que ele consegue sozinho aprender a usar, mas como eu disse, é a ponta do iceberg, a IA generativa, para mim, é um ponto de disrupção dentro do mundo da tecnologia e a gente vai ver coisa muito interessante acontecendo nos próximos meses, principalmente através desse ecossistema de inovação que eu comentei. O que eu acho muito interessante é que as pessoas sempre associam, a gente abriu o podcast aqui, a NVIDIA com as placas de vídeo e você fica lembrando do seu passado gamer lá, gente que é velha, mas eu tenho visto muita gente falando isso, cara, tem que olhar para o software da NVIDIA, que é o que realmente tem se destacado, até quando você vê análise de mercado e etc, todo mundo, quando o Paulo abriu o podcast e falou, ah, eu vou ver o que está bombando, aí eu achei que você ia falar das ações da NVIDIA, porque a ação da NVIDIA é o que bombou esse ano, e é curioso porque as análises superficiais olham e falam, ah, mas peraí, a ação da NVIDIA bombou um monte, mas o número de GPUs, versos, sei lá o que, e outros fabricantes de hardware não bombaram no mesmo tempo, por quê, né? E achei bacana porque o João Marco comentou exatamente o ponto que eu vejo muito o pessoal falando, cara, tem que encarar a NVIDIA como, além das GPUs, né, todo esse ecossistema de software e etc, que eu, inclusive, conheço muito pouco, até se você puder contar mais sobre isso, mas encarar a NVIDIA realmente como uma empresa no epicentro dessa revolução toda aí, né, que já estava acontecendo há muito tempo, como você falou. É, é muito interessante essa área de software, porque a gente fez um trabalho muito grande, né, durante os últimos anos, de conversar com o usuário final da tecnologia para entender a demanda específica dele, e aí descobrir como é que a gente atende aquilo com software e também com hardware. Então, existem diversas alterações que foram feitas nas GPUs nos últimos anos, que vieram exatamente identificar essa necessidade de um novo tipo de computação. Um exemplo disso são os transformers, que é a tecnologia básica por trás de processamento de linguagem natural. Hoje, os transformers começam a ser usados em visão computacional, e vai ser muito disruptivo.o que isso vai permitir, e o que nós fizemos é que o Transformer antes era processado por software, hoje a gente tem um circuito dentro da GPU que processa Transformers, o mesmo vale para Tensor Cores e uma série de outras tecnologias. Uma GPU moderna agora da NVIDIA, ela executa Transformer em hardware, é isso? Exato, executa Transformer em hardware, por isso que tem uma diferença medonha no processamento. E como eu falei, Transformer está entrando para a visão funcional e redes neurais que trabalham com áudio, vídeo e texto junto, então isso tende a crescer bastante. Mas uma coisa interessante de comentar é o seguinte, a gente não tem só os nossos SDKs, nós trabalhamos também com diversos projetos open source e distribuímos versões desses projetos prontas para uso tanto no desenvolvimento quanto em produção, permitindo que a aceleração possa ser utilizada. Então, para dar um exemplo disso, o TensorFlow ou o PyTorch, que muita gente utiliza, você pode baixar de diversos lugares o TensorFlow, o PyTorch, ter lá as suas versões, colocar as otimizações de software, mas se você quiser simplificar a sua vida, existe um site da NVIDIA, ngc.nvidia.com, que a gente disponibiliza gratuitamente um container já com o PyTorch e outro já com o TensorFlow, com todas as bibliotecas de aceleração de NVIDIA que você precisa para chegar com esse processamento otimizado. E a gente atualiza esse container pelo menos uma vez por mês, então é um container production grade, simplesmente completamente otimizado por NVIDIA. Além dos nossos próprios SDKs, a gente tem esse trabalho de customização e de integração com ferramentas de mercado. E é muito interessante ver o resultado prático disso. Eu já vi casos de universidade, por exemplo, estar usando uma versão do TensorFlow lá e levar três dias para fazer um processamento. Adotando a utilização desses containers com a nossa orientação sobre arquitetura, três dias se transformaram em cinco horas. Então a nossa preocupação é, de fato, ajudar o ecossistema a utilizar melhor as OPs. E através desse portal aí que eu comentei, do NGC, a gente disponibiliza container para diversas coisas. Então, para visão computacional, para processamento de linguagem natural, para ciência de dados. E ciência de dados é algo muito interessante porque nós não apenas damos a roda, a gente criou bibliotecas funcionalmente e semanticamente compatíveis com as bibliotecas do PyData. Então, Pandas, NumPy, Scikit-Learn e afins. Existem bibliotecas da NVIDIA que, dependendo do teu código, é só você trocar o include e você já está acelerando na GPU. Isso é um projeto open source chamado Rapids. Muitos dos frameworks da NVIDIA hoje são open source e todos os frameworks que a gente tem são gratuitos. Você pode até, se quiser suporte da NVIDIA, contratar através de uma licença específica de software chamada AI Enterprise. Mas todos os frameworks que a gente tem hoje da NVIDIA podem ser baixados gratuitamente e ser utilizados. Então, esse ecossistema de inovação é que faz toda a diferença para a gente. E como eu citei, aqui na América Latina a gente tem empresas fazendo coisas absolutamente sensacionais. Só nessa semana, e olha essa feira, eu conversei com duas empresas que trabalham com processamento de linguagem natural que tem soluções compatíveis ou do nível do chat GPT, mas em português. Existem empresas aqui no Brasil que já estão trabalhando com avatares. Não é só a criação de um algo igual ao chat GPT, é como colocar a interação daquilo com áudio e vídeo em tempo real, interagindo com as pessoas. Então, aquele gap que existia lá quando eu estava na faculdade, na década de 90, de que a tecnologia levava meses ou anos para chegar no Brasil, ele não existe mais. Então, grande parte das inovações que a gente vê acontecendo no mundo inteiro com inteligência artificial, eu acompanho isso no dia a dia na América Latina e está acontecendo aqui sim. E como eu falei, isso é possível porque nós conseguimos desenvolver um conjunto de software e um ecossistema para que qualquer pessoa que queira usar inteligência artificial hoje possa para desenvolver e com base nisso que está chegando aos usuários finais. É interessante, Gilmar, você fala com tanto conhecimento e é muito impressionante essa amplitude que tem, desde o suporte documentação, container já pronto. Olha, essa biblioteca substitui a outra que você já usa com a mesma API, com a mesma interface, só que já paralelizando, já usando todas as contas vetoriais, multiplicando tudo em paralelo. Só para citar um caso, Paulo, tem uma engenheira que trabalha aqui na NVIDIA, a Patrícia, que antes de entrar na NVIDIA, ela é especialista em data science e NLP pra caramba. Já está convidada, Gilmar. Ela já está convidada para o Hipsters aqui. É, ela trabalhava como cientista de dados numa empresa e ela estava fazendo um treinamento com um algoritmo chamado XGBoost lá para detecção de fraude. Então ela conta que eram em torno de 15 horas cada batch de treinamento dela. Então olha como era produtivo o dia dela de desenvolvimento, ela ia botar o batch para rodar perto da hora do final do expediente, deixar ele rodando durante a noite inteira. No dia seguinte, lá pelo horário do almoço, tinha terminado de rodar e ela começava a fazer os testes. E aí ela descobria, putz, eu precisava mexer isso no data set, eu podia mexer nesse hiperparâmetro, não está dando a curácia que eu quero, a precisão não está tão interessante, porque é assim, é exploratório o trabalho de treinar a rede neuronal em geral. E aí ela montava um conjunto de hipóteses de melhoria, esperava dar o final do dia e botava para rodar de novo. Então como você deve imaginar, eram semanas para desenvolver alguma coisa. E aí ela conheceu essa page Data Science, que é a Rapids, né? Ela migrou o código dela para o Rapids e começou a fazer testes na nuvem. Sem usar a GPU mais parruda que ela poderia ter acesso, o tempo de treinamento dela de 15 horas virou 15 minutos. Então a solução que ela desenvolvia, a solução que ela desenvolvia em semanas, ela passou a conseguir entregar em questão de dias. E assim, ela teve que reescrever o código todo. Foi uma adaptação que foi feita. As APIs são muito similares, muito fáceis de utilizar, né? Inclusive quem tiver interesse, o site é rapids.ai, é um projeto open source e uma coisa legal que colocaram recentemente lá, qualquer desenvolvedor que acessar ali, vai ter uma área que ele pode experimentar esse framework imediatamente dentro dos Cloud Providers. Então você dá dois cliques ali, faz seu login no seu CSP e você, por exemplo, com o Google Web, já consegue ver em tempo real o código rodando, pegar o teu código, subir para lá e ver a diferença que ele vai ter de performance. Então, assim, realmente é muito acessível, né? Se a gente pegar a área, por exemplo, de visão computacional, que é uma área que eu estudo já há muitos anos, né? E a gente tem uma massa, um volume gigante de desenvolvedores que trabalham com OpenCV, com TensorFlow, etc. Esse pessoal, quando começa a usar o DeepStream, que é o framework da NVIDIA, consegue dominar o framework, assim, em questão de dias. Porque quem já aprendeu a fazer do jeito difícil, que é o jeito que a maior parte dos desenvolvedores usam hoje, na hora que pega um framework como o DeepStream, rampa nele muito rápido, porque ele realmente simplifica grande parte do desenvolvimento. E aqui uma dica importante. Por que que eu vou usar um framework da NVIDIA e não as ferramentas que eu entendo ou que eu quero para fazer uma aplicação? Existe um custo funcional muito grande cada vez que você precisa tirar o dado da memória RAM, levar ele para a memória da GPU e devolver. Então, se você precisa usar uma GPU em um pedaço do seu pipe de processamento, como é o caso da inferência, para você maximizar a performance dessa GPU, o ideal é que você faça a maior parte da computação dentro dela. Então, a gente pega o dado uma vez da memória RAM, traz para dentro da GPU e aí eu consigo processar todo o pipe de IA dentro da GPU e só devolvo o dado final. Essa é a situação ótima para você utilizar bem uma GPU. E é exatamente para isso que nós desenvolvemos todos esses frameworks. Uns falam com os outros e eles falam com frameworks de mercado também através de contribuições de projetos Open Source e código da NVIDIA. Então, o que a gente busca no final do dia, sendo bem técnico, sem entrar na nerdice absurda, é permitir que os desenvolvedores consigam processar um pipe completo de inteligência artificial dentro da GPU. O ideal é ele buscar o dado no início do processamento, ele processa tudo na GPU e devolve o dado no final. Esse problema que eu falei da latência de troca de dado em memória, ele não é exclusivo de GPU, isso acontece com qualquer outro tipo de hardware que está na adjacência do computador e isso acontece dentro das CPUs também. Então, essas otimizações são super importantes para que a gente possa fazer mais com menos. Nós não estamos preocupados em vender as GPUs top de linha para todo mundo. Lógico que a gente gostaria que isso acontecesse, mas a nossa preocupação, principalmente no trabalho do NVIDIA Inception, que é o nosso programa de startups, é ajudar as startups a montar uma arquitetura que seja escalável, ou seja, hoje eu começo pequeno, amanhã eu não vou estar gigante, não vou ter que reescrever tudo, e principalmente ajudá-los a utilizar a GPU que viabiliza o negócio deles, porque aquela ideia do passado de que vou pegar o hardware mais caro e mais rápido e vou trabalhar, ela não vale para startups que têm recursos de investimento muitas vezes limitados. Então, é extremamente importante aprender a usar bem o hardware e não dar passo maior que a perna. Esse é um trabalho que a gente faz com as startups. Quem quiser conhecer mais do programa aí, nvidia.com.br startups. Se tiver uma startup, se o seu amigo tiver uma startup e trabalha cunhado com aceleração, se inscreva no programa Inception, que a gente ajuda as startups exatamente com tudo isso que eu comentei e mais uma série de outros benefícios interessantes lá. Mas como vocês veem, é um trabalho de ação.ecossistema. Então, sem esse ecossistema de software, que começa na NVIDIA, mas é composto por milhares de empresas do mundo todo, a gente não daria onde a gente está hoje, com o I.A. e com essa revolução toda. É um ecossistema de inovação aberta, de verdade. É interessante fazer a analogia com as placas de vídeo de videogame, de jogos, que chegou numa complexidade, num nível de sofisticação, a NVIDIA, que tinha, inclusive eu diria que maior, que tinha na época que começaram a vir os OpenGL, o DirectX e as ferramentas da NVIDIA para otimizar o seu jogo e as ferramentinhas daquela placa de vídeo. É óbvio que isso era muito para o consumidor final, mas mesmo assim, tem esse ecossistema grande e rico da NVIDIA para esse tipo de placa. Aí agora chega e a gente, eu que não conheço esse assunto, vendo o que o Jomar está trazendo aqui, o tamanho que esse ecossistema virou, é óbvio, porque também se tornou um dos principais motores econômicos dentro da própria empresa, mas para você ver que a estrutura criada para atender essa demanda é enorme e chegou não só a criar os próprios drivers, chegou a criar os próprios, reescrever os frameworks usando a mesma API que já tinha, para que as pessoas pudessem facilmente trocar os motores, não é? Ao contrário, ficava o que acontecia 20 anos atrás no desenvolvimento de jogos, ou até um pouco mais, que as pessoas precisavam reescrever o jogo para rodar no outro videogame, na outra placa de vídeo, não é? Aí essas camadas vêm vindo, mesmo que elas não tenham sido definidas, né? Ninguém falou PyTorch é uma interface, é uma especificação. Não, isso foi ganhando o mercado e depois que ganhou o mercado, todo mundo falou, não, pera lá, se alguém quer criar alguma coisa, já usa a API do PyTorch ou do Scikit-Learn, porque tanta gente usa, já virou o standards de facto, não é? É o padrão, o padrão nasceu, aconteceu, né? Surgiu uma tomada, surgiu um padrão de tomada que está todo mundo usando logo, todos nós vamos usar, inclusive nós da NVIDIA, vamos deixar que você encaixe a tomada nesse mesmo formato, para que a gente aproveite melhor e que sua empresa, seu projeto, seja muito bem recebido. E tem mais um detalhe que é importante, para a gente, pouco importa onde essa GPU está, para mim pouco importa se a GPU está dentro do Google, se está na AWS, se está num data center local, se está embarcado num dispositivo de internet das coisas ou numa câmera inteligente, a gente está preocupado com a otimização do software, então a gente não tem essa preocupação de onde a sua GPU vai estar. Isso é um desafio técnico, porque a gente precisa desenvolver softwares e frameworks que suportem esse tipo de deploy, principalmente para alguns frameworks que você pode ter a opção de fazer o deploy desde uma plataforma embarcada, como é a Samsung, que é muito usado em robótica, em smart cameras, em uma série de aplicações, e essa mesma aplicação consegue, se você consegue fazer o deploy dela, na nuvem, e o detalhe, com otimização para a GPU que você vai rodar. Então, o ferramental completo que a gente tem para os desenvolvedores, ele tem essa abrangência. Uma vez que eu tenho um projeto desenvolvido usando esses SDKs da NVIDIA, que eu repito, são gratuitos, eu tenho total flexibilidade de como eu vou fazer o deployment da solução que eu desenvolvi. No cliente A, eu vou processar na nuvem, por exemplo, para visão computacional, que é um caso comum. No cliente A, a conexão de internet dele é ótima, ele tem redundância, então eu vou pegar o circuito interno de TV dele, vou fazer streaming disso para um cloud provider e vou processar a visão computacional lá. No cliente B, ele não tem condições de ter uma internet com essa qualidade, então eu vou precisar fazer o processamento lá dentro da loja dele junto com a câmera. E aí, eu tenho N opções, colocar um servidor, colocar uma placa Gex embarcada, enfim, o que houver no orçamento do projeto. Mas o ponto importante é, o mesmo software que eu roto na ponta, eu vou rodar no servidor. Eu não tenho que sair reescrevendo ele para cada vez que eu vou fazer um deployment diferente. E isso faz toda a diferença para quem realmente quer escalar as soluções. E quando eu falo em escalar, eu tô falando aí de aplicações como uma que a gente tem na América Latina, meu parceiro, com mais de 40 mil câmeras. Então, imagina você processar, em tempo real, de mais de 40 mil câmeras de vídeo, fazendo associação de tudo quanto é tipo de dado que você tá recolhendo daí. Então, esse é o exemplo de escalabilidade que a gente consegue utilizando as nossas tecnologias. E a nossa preocupação no apoio às startups é exatamente essa. Eu trabalho já com startups desde 2016, numa vida passada na área de tecnologia, e muitas vezes eu encontro startups com ideias geniais, com produtos, com funcionalidades muito interessantes e com valor muito grande, mas que por trás do capô, a arquitetura era completamente complexa e quase impossível de escalar. Então, a startup começa crescendo, no momento em que ela acerta na veia e dá aquele boom de crescimento, a tecnologia dela inteira capota, porque ela não foi preparada para ter escalabilidade. E essa é a nossa grande preocupação quando a gente conversa com as startups e as orienta para inteligência artificial. Como vocês viram aí, o ChartGPT foi a aplicação que conquistou o maior número de usuários em pouco tempo. O que eles fizeram em dois meses, teve coisas como Instagram, TikTok, que levou na base de anos para ter. Então, a escalabilidade e a arquitetura escalável é extremamente importante. Por isso que engenharia de software é um negócio para mim fundamental, para trabalhar com essas coisas, e o mais legal, o trabalho com comunidade. A gente não precisa mais ir para dentro de uma universidade, fazer um mestrado ou um doutorado para trabalhar com o IA e para aprender esse monte de coisa que eu estou falando. Conseguimos fazer com que comunidades de tecnologia chegassem nesse nível hoje, que as pessoas conseguem aprender simplesmente trabalhando junto com comunidades. E esse tema da inteligência artificial e também o tema da comunidade, você está ouvindo a gente, está vendo que o Jomar está colocando todo o trabalho da NVIDIA e dele, fazer com que seja mais acessível o próprio produto. Então, esse é um grande desafio e também para a empregabilidade. É um grande desafio das empresas que consomem, usam, criam muita tecnologia, aparecer muito bem na comunidade e saber conversar com o mercado de pessoas desenvolvedoras. Me parece que todo esse caminho que a NVIDIA está construindo, desde API até mecanismos de você participar de um grupo, é isso de relação com desenvolvedores e desenvolvedoras, o DevRel. Cada pessoa acaba fazendo a sua própria definição, mas enfim. E o Jomar vai estar lá no DevLeaders, que é dia 11 de agosto em São Paulo, e o talk dele vai ser focado em comunidade. Que Jomar é interessante porque aqui no Hipsters, que é uma comunidade, o podcast é uma comunidade que a Alura criou, cria, mantém e sustenta, é algo que muita gente vem buscar a gente. Muita gente de liderança de empresas de tecnologia vem me perguntar assim, poxa, mas como que eu faço para minha empresa ser conhecida entre devs? Ou, mais complexo ainda, que é o seu caso, como que eu faço para que devs considerem o meu produto no momento de compra? Considerem usar o meu produto? Então empresas que criam produtos para devs, seja de IDEs, sistemas de gestão, de controle de hora, de cloud e tudo mais, essas pessoas, essas empresas, e não necessariamente só as americanas, as outras, porque é o caso da NVIDIA, é o caso das providores de cloud, todas as big techs, elas precisam criar um vínculo com a comunidade. Se esse vínculo não é criado, é muito mais difícil de você gerar aquele buy-in, aquele mecanismo de, peraí, já tem muitas pessoas querendo esse produto, agora só falta o tomador de decisão da martelada final, que vai usar a NVIDIA, que vai usar a Alura, que vai usar tal cloud, que vai usar tal editor, que vai usar tal sistema de gestão de conhecimento. Então o seu trabalho está cada vez mais em alta, né? A gente já fala aí há alguns anos, mas está cada vez mais em alta e você vai trazer esse assunto no Dev Leaders, é isso, Gilmar? Isso, a ideia é falar, o que eu quero compartilhar no evento, né? São as minhas experiências trabalhando com comunidade ao longo dos anos. Eu comecei a trabalhar com comunidades há exatos 20 anos, em 2003, e desde 2014. Então é engraçado que Dev Relations nasceu depois disso, não tinha esse nome, era uma coisa meio esquisita, tinha um pé no marketing, um pé na engenharia, um pé em suporte, etc. E como você falou, hoje, empresas de tecnologia é fundamental ter esse relacionamento. Uma confusão que fazem muito é achar que Developer Relations é para atrair talento para a empresa. Quem faz isso é a área de talent acquisition e tem ferramentas específicas para isso. A ideia do Developer Relations é ajudar a jornada do seu desenvolvedor na sua tecnologia. E pouco importa se ela é um produto, se ela é uma biblioteca, se ela é um serviço. No final do dia, você colocou muito bem, Paulo, se você não tem alguém que está disposto a usar aquilo, você não vai conseguir crescer. Então, encontrar essa comunidade, esse é o ponto, né? A gente não cria comunidade, as comunidades existem. O que a gente tem dentro de Dev Relations são aprendizados e experiências de como engajar essas comunidades que existem e transformá-las ou juntá-las em algo novo, que aí você consegue ter esse grau de interação grande, né? Mas não importa o tamanho da tua empresa, se ela é pequena ou se ela é grande, você não vai escalar um produto ou um serviço sem ter esse trabalho de comunidade. Então, o trabalho de Developer Relations é esse. Jomar, acho que você colocou muito bem. Eu usei o termo criar o podcast, mas a minha visão é muito na sua direção, porque toda vez que alguém fala, não, eu vou criar uma comunidade para a minha empresa, não, espera lá. Pode até acontecer, mas isso é um de mil. O ideal é que você se envolva nas comunidades existentes, você pode até criar um subfórum, um podcast novo, etc. Mas não baseado na sua empresa. Tanto que o maior exemplo aqui é o...podcast ou o fórum que a gente tem o guji eles não se chama lura cast e eu acho que as pessoas cometem muito esse erro em comunidade de empresas elas criam a empresa cast e não percebem que aí elas estão conversando só com elas mesmas ou pelo menos no início é claro se você é uma coca-cola ou até mesmo nvidia muda um pouco esse perfil você consegue um pouco mais esse espaço dada a notoriedade das pessoas que trabalham aí mas mesmo assim é muito mais interessante no caso da nvidia se envolver com a comunidade python open source que já é muito maior estruturada do que vamos fazer esforços do zero com controle total e absoluto aí não é comunidade aí é um sistema dentro da empresa que não é ruim mas é outra coisa é um outro mecanismo serve para outro propósito e comunidade tem um treco interessante que é o seguinte a comunidade ela está muito dependente de cultura então o que eu vou mostrar na talk lá é o que é que eu aprendi nesses últimos 20 anos para conseguir pegar um plano global de engajamento com comunidades e transformá-lo em algo que funcione na américa latina já tive oportunidade de fazer isso na áfrica também e hoje na realidade dentro da nvidia eu trabalho muito com o pessoal de países emergentes também então a gente troca muita experiência em relação a isso porque muitas vezes o que atrai um desenvolvedor americano não é a mesma coisa que atrai um desenvolvedor brasileiro argentino peruano então entender onde essas pessoas veem valor onde ela sentem dor e como é que você ajuda nisso é extremamente importante porque assim que o relacionamento se abre segunda coisa é que comunidades têm regras escritas e têm regras não escritas então não existe aquela história de um jeito de trabalhar com comunidades funciona para todas existem comunidades que adoram estar próximas de empresas existem comunidades que não gostam de ter essa relação tão forte com empresas então conseguir entender como é o comportamento de cada lugar desse de cada comunidade dessa como é que você pode de fato né ajudar a comunidade naquilo que ela está precisando naquele momento é fundamental então eu vou compartilhar no evento uma série de aprendizados que não foram só meus até essa essa tal que eu vou fazer lá um pedaço dela praticamente a ideia dela toda na verdade eu desenvolvi junto com o neto marinho do google é que há um evangelista também bem antigo aqui do brasil da américa latina foi muita troca de informação com ele com outras pessoas ao longo dos anos e tem muita coisa que eu vou falar ali que eu aprendi quebrando a cara mas tem muita coisa que eu vou falar ali que eu aprendi vendo os outros quebrar a cara e depois de conversar para entender o que tinha acontecido então isso acontece não é interessante de dar relations é que eu diria que não é uma ciência exata né não existe nada escrito na pedra a gente tem eventos globais aí para discutir exatamente isso todo ano eu palestrei até no evento global no ano passado e a gente vai testando experimentando e aprendendo é um trabalho muito gostoso porque ele é dinâmico então sociedades e culturas se modificam o passar dos anos e a gente tem que se adaptar a isso um exemplo é a pandemia quando a pandemia começou é basicamente tudo o que a gente tinha de ferramenta para trabalhar com dev relations foi jogado no lixo e a gente teve que reinventar tudo e a gente reinventou e a gente chegou hoje num modelo que pra mim é muito mais efetivo quem é que hoje imagina fazer um evento presencial que não vai ter nenhum pezinho no mundo da internet não dá mais pra fazer isso no passado isso era o normal não vou nem transmitir a minha palestra daqui porque quem tá aqui vai assistir hoje em dia o contrário você tem que expandir sua audiência você tem que entender que as pessoas não vão estar na região específica em que você está dependendo do que você quiser falar então para a empresa saber como montar uma estratégia de engajamento saber como medir e saber enxergar o que está dando certo que dá dano errado antes de investir um rio de dinheiro é fundamental e é sobre isso que eu quero compartilhar e comentar lá no evento é pra galera que quiser trabalhar com comunidade tem empresa de tecnologia não sabe exatamente como começa pra galera que tem vontade de trabalhar no brasil é na área de dev relations de multinacionais principalmente na américa latina eu recomendo bastante esse bate-papo porque ali eu vou compartilhar muito do que eu aprendi vou irritar algumas pessoas com as coisas que eu vou falar lá principalmente o pessoal de talento aquisitivo e de marketing mas aí eu deixo pra quem quiser ficar irritado e lá o vivo ver e mas no geral eu acho que vai ser uma vai ser um bate-papo muito legal pra poder compartilhar essas experiências todas né e no final do dia a jornada do dev ral é acompanhar o desenvolvedor desde o momento em que escuta falar de alguma tecnologia até o momento que ele botou a aplicação em produção com aquilo eu sou apaixonado por fazer isso como falei faz 20 anos que faço eu acho isso fundamental para qualquer empresa de tecnologia hoje sérgio e você no dev leaders você esteve no dev leaders ano passado e estará esse ano acredito com o guilherme silveira cofundador da lura e pelo que eu entendo tem inteligência artificial no seu assunto também você que está sendo o capitão aí junto com gui junto com outras pessoas na lura no podcast novo de inteligência artificial em ter criado nossa própria inteligência artificial né com a open ea e que a luri que está entrando aí em beta para os nossos alunos e alunas do plano pro que que você vai trazer no dev leaders e deixar seu convite aí para que as pessoas participem em são paulo queria entender já um pouco do spoiler dos seus aprendizados e como líder não é como gestor aí de um time de quase 100 pessoas que as pessoas que estarão lá no evento podem aprender com você e com o guilherme silveira então em agosto eu e o guilherme silveira vamos apresentar lá no dev leaders sobre estratégias para adoção de ar nas equipes tá bom então o que a gente quer tentar trazer um pouco é na prática como que a gente começa a transformar o trabalho das equipes para obter mais produtividade a partir dessa revolução nova das eas ok então a gente tem tido muitas experiências aqui internamente na lura tanto eu quanto o guilherme com os times que a gente está mais envolvido com centenas de pessoas e tudo isso é realmente procurando transformar o dia a dia do trabalho delas ok então assim de uma maneira mais geral a nossa visão pra ia né tem muita gente hoje é desesperada com a apocalíptica com a nossa visão geral das eas é que ela vem para potencializar o trabalho das pessoas não para substituir certo é claro que pessoas mais produtivas são uma transformação na economia e na sociedade então vão ter impactos sociais evidentemente mas de maneira geral que a gente enxerga é que esse grande produtividade vai se assentando e vai melhorando o mundo como um todo né então o que a gente quer mostrar é como que a gente viu cenários nossos e de pessoas que a gente tem próximos que estão utilizando isso em equipes em larga escala para aumentar a produtividade então a gente tem um contexto aqui de uma edtech que é a lura né onde a gente tem time de mais variados tipos a gente tem time de educação realmente professores professoras instrutores que que preparam os cursos e tudo mais e a gente tem time de tecnologia que trabalha com a plataforma a gente tem time de design e que trabalha com criações a gente trabalha com time de marketing comunicação e etc né então é um microcosmos aí de muito que tem visto em outras empresas também que tem áreas parecidas ok a gente quer mostrar como que a gente está usando hoje e como foi a implantação e a criação de certas ferramentas baseadas em ai para a produtividade interna para aumento da efetividade dos colaboradores então a gente vai mostrar como que os nossos desenvolvedores estão utilizando como que os nossos professores estão utilizando e assim por diante então acho que tem um pouco de cultura de implantação de liderança né acho que a gente vai mostrar barreiras barreiras iniciais que as pessoas têm a gente vai mostrar como que a gente seguiu isso para ter uma cultura de realmente trabalhar junto e todo mundo enxergar o ganho de produtividade a gente vai mostrar também um pouco da parte técnica não no sentido de mostrar código mas no sentido de mostrar como que você customiza o que você precisa fazer para ter ferramentas mais adequadas para o trabalho ali das pessoas então não basta dar uma assinatura do chat gpt para todo mundo e falar tá bom beleza agora faz o seu trabalho melhor né isso passa por mapear os fluxos de trabalho enxergar as oportunidades onde a IA pode ajudar ou não ter especialistas pessoas que conheçam de IA corretamente para poder poder guiar o modelo a gente sabe que os modelos especialmente de IA generativa podem ainda alucinar bastante então guiar os modelos, obter resultados, conseguir medir esses resultados, ouvir o feedback dos times que estão trabalhando em cima dessas ferramentas e tal tudo isso simples tá não é nada do outro mundo nada inacessível aí para as empresas de tecnologia no geral então a gente quer mostrar os resultados que a gente obteve que foram incríveis, alguns resultados bem bons, alguns fluxos de trabalho nossos que caíram de dias para horas por exemplo e os feedbacks dos times das pessoas que estão trabalhando que realmente enxergam isso como um ganho de produtividade então o foco da palestra é esse a gente vai conversar lá eu e o Guilherme a gente está preparando um conteúdo bacana a gente vai conversar lá com todo mundo sobre isso sobre a aplicada aos times das empresas de tecnologia e como aumentar produtividade e eficiência né usufruir dessas coisas novas de uma maneira não apocalíptica mas realmente trabalhando junto e obtendo resultados bons. Então fica aqui o meu convite para todo mundo participar do Dev Leaders que é esse evento aqui em São Paulo presencial quem puder vir em agosto além das palestras que a gente citou hoje tem várias outras sobre tópicos que permeiam em liderança, liderança técnica, empresas de tecnologia e assuntos importantes para os times dá uma olhada lá no site dá uma olhada nanas pessoas que já estão como palestrantes lá, tem muitos nomes bacanas, muitos temas bacanas. Acredito que vai ser muito rico aí para muitas pessoas, então fica aí o convite para todos venham participar do Dev Leaders no comecinho de agosto. Bem, eu queria agradecer o Jomar e o Sérgio pelo episódio podcast e porque também estarão lá no devleaders.com.br que os ingressos já estão no terceiro lote, dia 11 de agosto, que é um evento desenvolvido pela Lura e tem muita gente da comunidade, pessoas de perfis diferentes, de senioridades diferentes que com certeza você vai se relacionar com os diversos assuntos de pessoas que estão em gestão e liderança de equipes de tecnologia. E a gente tem um encontro na próxima terça-feira. Hipsters, abraços, tchau! Se você já tem um caminho na carreira de tecnologia e está trabalhando com liderança, está trabalhando com gestão de time ou como tech lead, ou tem muito interesse nessas carreiras onde são envolvidas também outras habilidades o evento Dev Leaders Conference vai acontecer dia 11 de agosto. Se você entrar lá no devleaders.com.br, você vai ver todas as informações desse evento que acontece em São Paulo que é a segunda edição, que a Lura está tocando e a gente já tem palestrantes incríveis. Mas ainda é um ambiente para você trocar ideias sobre gestão, sobre lideranças e sobre todas as outras carreiras que estão em volta do desenvolvimento de software. Eu te espero lá no Dev Leaders. Este podcast foi produzido pela Alura, Mergulhe em Tecnologia e Faculdade FIAP Let's Rock the Future, edição e sonorização, radiofobia, podcast e multimídia.