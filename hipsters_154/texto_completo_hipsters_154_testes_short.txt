Hipsters.tech, o podcast de tecnologia e outras modinhas. Olá, caríssimo ouvinte. Seja bem-vindo a mais um episódio do seu podcast favorito. Esse aqui é o Hipsters.tech, meu nome é Paulo Silveira e hoje a gente vai conversar sobre testes automatizados. Então se você já está testando ou quer testar e quer conhecer novas abordagens em testes automatizados para garantir que seu software está funcionando e que vai continuar funcionando depois das milhares de mudanças desse sprint, aproveita para ouvir a conversa de hoje. Vamos lá ao podcast e ver com quem que a gente vai falar. E para essa conversa de hoje eu estou com Maurício Anischi, com quem já tive a honra de trabalhar aqui na Kaelon Alura e que hoje em dia é professor e pesquisador em engenharia de software lá na Holanda, na Universidade de Delft. Como você está, Anischi? Oi, Paulo, tudo bem? E junto com a Anischi eu estou com os co-hosts para gerar polêmica. Primeiro estou com a Roberta Arcoverde, que trabalha nesse Tech Overflow, famosa por ter pouquíssimos ou nenhum testes automatizados. Como você está, Roberta? Constrangida. Bem constrangida. Hoje por acaso acabei de subir um pull request com 400 linhas de código alteradas e zero linhas de teste. É só torcer um pouco. Eu fiquei com medo, na verdade. Eu fiquei com medo, Paulo, porque a hora que eu vi que a Roberta participou eu falei, meu Deus do céu, se o Tech Overflow não testa quer dizer que ninguém precisa testar, né? De jeito nenhum, pelo amor de Deus. Aí vão dizer que eu estou promovendo essa ideia de testar e do tipo, não, gente, é terrível eu trabalhar sem teste. Eu estou assim, não posso falar palavrão aqui, mas estou com coisas na mão, assim, com esse PR. Tenho que entrar na master E também com o nosso Maurício Balboa Linhares, diretamente da Filadélfia. Como você está, Linhares? Opa, tranquilo. Quero ver como é que a gente vai sair do Extreme Goal Host aí, né? Vamos fazer teste, vamos botar para rodar na produção. Como é que é esse negócio aí? Como é que funciona esse negócio? Olha só, para dar um contexto, o episódio do Hipsters.tech de número 51 foi sobre testes automatizados. Curiosamente, também com Maurício e a Anish, onde a gente fala um pouco sobre o que é teste. Começamos lá com o J-Unit, com testar primeiro antes de escrever o código e etc. Então, acho que hoje a gente queria dar um passo a mais e falar de novas abordagens de testes automatizados, como que as pessoas que têm zero ou quase nenhum teste podem tentar resolver os seus problemas e o que vem por aí nas pesquisas e nas universidades. O que você pode falar para a gente, Anish? O que eu queria falar é que a gente, como indústria, como desenvolvedor, a gente deu um passo muito bom, né? Que a gente começou a escrever teste. Todo mundo hoje sabe o que é J-Unit, o que é o seu framework de teste automatizado, todo mundo sabe o que é TDD. Isso foi muito legal, né? Porque a gente saiu do teste 100% manual para um teste um pouquinho automatizado, né? Então, você escreve o seu teste lá, você programa ele, aí você aperta um botão, a máquina executa para você, né? A galera chama isso de teste automatizado, né? E isso é teste automatizado. Mas o engraçado é que na academia, quando você fala para alguém teste automatizado, o cara não está pensando só na execução do teste, ele está pensando também na criação do teste. Ou seja, a máquina escreve o teste para você. Esse seria o último passo se a gente quer falar de teste automatizado, né? Não é a máquina só executar, mas é a máquina também desenhar o teste. Acho que no podcast anterior eu falei muito sobre como pensar em ensinar de teste, o que fazer. Os meus livros, os meus cursos na Kaylan eram super focados nisso, né? Ou seja, tinha o humano ainda na jogada. E a ideia do podcast de hoje é justamente tentar pensar em maneiras de remover o humano completamente do processo de teste. Parece mágico, parece uma promessa difícil de cumprir. Sabe que é uma ideia que não é tão recente, né? Porque eu lembro quando eu estava na graduação, tinha uma pesquisa do Universidade Federal de Campina Grande, que existia um plugin para Eclipse na época que gerava casos de teste no JUnit para testes de unidade, mas eles eram muito primitivos ainda no sentido de que, por exemplo, se era um teste, um código que recebia o inteiro como parâmetro, ele gerava os casos para inteiro negativo, zero, o máximo valor do inteiro, esse tipo de aleatoriedade, né? Mas com árvores mais complexas de objetos, eu lembro que não só era mais difícil de gerar os casos aleatórios, como também mais custoso do ponto de vista de performance. Agora sim, né? Claro que isso faz 11 anos já, então eu imagino que o estado da arte, que as pesquisas nesse ponto hoje em dia estejam bem mais avançadas e mais úteis e mais fáceis de utilizar no dia a dia. Olha só, eu e o meu amigo pessoal, Gustavo Soares, que trabalhou nesse projeto por muitos anos lá na UFCG hoje. Hoje ele tá na Microsoft, ele não tá trabalhando diretamente nessa área, mas ele tá trabalhando em coisas relacionadas e ele trabalhou por muitos anos escrevendo e uma das coisas interessantes que eles fizeram foi uma ferramenta que ela gerava um caso de refatoração e colocava o Eclipse para executar a refatoração e validava para ver se a refatoração que o Eclipse executou realmente era correta, né? Porque tem vários casos onde você pode achar que você fez a refatoração e tá correto e a sintaxe pode estar correta, mas o código não vai compilar porque tem outros problemas. Então eles trabalharam nessa. É uma solução que é muito parecida com as coisas que a gente fala de mutation testing, né? Que você cria uma coisa diferente no código e espera que o código tenha sucesso ou ele falhe, dependendo da mudança que você fez. Então é uma pesquisa que continua hoje, eu acho que ainda continua hoje lá na UFCG, ele não tá mais lá, mas ele trabalhou por muitos anos nisso aí. É uma coisa assim que no caso deles era um pouco mais simples porque eles tinham um caminho finito, né? O caso era testar todos os casos de refatoração do Eclipse, mas para uma coisa assim, remover completamente o desenvolvedor escrevendo testes é um passo um pouco maior, né? É um negócio meio grande quando você abre para todos os casos possíveis que o usuário pode chegar e pode fazer alguma coisa lá. Qual que é o estado disso hoje em dia? Tem coisas que eu já posso usar? O que eu já consigo criar de testes dado que eu tenho um código que funciona e eu faço testes para ele? Tem alguma coisa meio mágica que já funciona minimamente bem? Sim, o desafio é muito grande, né? A Roberto e o Maurício comentaram de desafios, mas as coisas estão melhorando, né? A gente tem aprendido cada vez mais sobre como tentar ensinar a máquina a escrever o teste para você, né? E tem bastante ferramenta, eu vou comentar, posso comentar alguma delas, mas acho que eu queria falar primeiro, de alguma forma, explicar o porquê que é difícil ensinar a máquina a criar para você, né? Acho que a grande dificuldade é que, no fim das contas, a máquina precisa saber o que o seu software faz para testar, certo? Se, sei lá, o seu software calcula quanto vai custar para entregar o produto que é feito em São Paulo, no Rio de Janeiro, a máquina tem que entender um pouquinho da regra de negócio e, puxa vida, como que você ensina a máquina, né? Você não ensina. É o que a gente chama, é o problema do oráculo. O computador não sabe o que o seu software tem que fazer para saber quais as sessões escreveram o seu teste do JUnit, né? Mas o grande truque nisso é que são de graça, né? O que eles fazem sentido em qualquer software, né? Então, se o seu software soltar uma Null Pointer Exception, não importa o que o seu software faz, certo? Uma Null Pointer Exception é um problema. Se o seu software soltar uma Array Index Out of Bounds Exception, né? Você tenta assar uma posição no array que não existia, não importa o que o seu software faz, isso é um bug, né? E é, mais ou menos, por aí que essas ferramentas têm caminhado, né? Então, elas tentam olhar para o seu código, olhar para o seu software, muitas vezes nem para o seu código, né? Se uma aplicação web tem ferramenta que só olha para o browser, né? Para o que está no browser, que ela sai explorando ali para tentar achar uma Null Pointer Exception ou um erro 500, sei lá, se você está testando uma aplicação web, né? Os pesquisadores têm melhorado muito a maneira de explorar para conseguir achar bug, né? E de maneira rápida, de maneira mais efetiva. A Roberta deu um bom exemplo, né? Dez, quinze anos atrás, quando a galera começou a pensar e, puxa vida, a gente consegue ensinar a máquina a testar, as primeiras ideias foram, puxa vida, sai fazendo coisa aleatória, né? Então, sei lá, eu tenho a minha classe matemática, que tem um método soma que recebe A e B, sai criando o teste, passando o elemento, valores aleatórios, né? Roda aí, por dez minutos, quem sabe você não ache um bug, né? E parece bobo, parece meio estúpido, mas tem bastante paper mostrando que só busca aleatória, como a gente chama, acha uma pancada de bug. Obviamente não o bug que talvez você queria achar, de que, puxa vida, estou calculando o imposto errado aqui no meu software de qualquer coisa, mas ele consegue achar uma Null Pointer Exception de uma maneira que você talvez não fosse conseguir, entendeu? Uma coisa que eu acho que a gente não consegue ver muito bem, na nossa maneira de pensar, é que, puxa vida, a gente faz software complexo, né? E a gente tá lá programando uma classe, às vezes duas, três classes juntas, a gente sabe um pouquinho do requisito, a quantidade de cenários que a gente consegue ver e explorar é muito pequena, né? A gente consegue basicamente testar o, aquilo que tá na nossa cabeça, a gente expressa de maneira, em forma de teste. A gente raramente pensa em, puxa vida, se eu combinar essa sequência de etapas aqui malucas, o meu software vai explodir. E esse tipo de ferramenta consegue explorar esse tipo de coisa, muito melhor do que talvez o ser humano conseguiria. No caso, isso seria uma extensão de coisas como o QuickCheck? Seria um gerador de entradas pra você rodar no seu teste, validar que o teste consegue sobreviver à execução de todas essas entradas? O QuickCheck é parecido, não tem muito inteligência artificial ali, né? O QuickCheck, ele espera com que você escreva propriedade, certo? Então, você define restrições daquela sua função, sei lá, minha função nunca pode retornar negativo e o QuickCheck tenta passar valores aleatórios, certo? Pra ver se aquela propriedade que você definiu no começo, ela se aplica sempre, né? Só que no QuickCheck mesmo, você ainda tem que definir a propriedade, né? Você tem que falar que essa função aqui, ela nunca pode retornar negativo. O próximo passo é fazer com que o software aprenda essa propriedade. Vou dar um exemplo pra você entender mais fácil como que esses algoritmos de inteligência artificial funcionam hoje. Vamos supor que você tem a sua classe Java ali, certo? Começando pensando só numa unidade, né? Pra depois a gente expandir pra alguma coisa um pouco mais complexa. Pensa lá numa classe. Sua classe tem lá uns ifs, tem uns fores, etc. Você com certeza conhece cobertura de código, certo? Você tem aqueles relatórios bonitinhos e você sabe que se você quer cobrir o seu código cem por cento, você tem que explorar o if quando ele é verdadeiro, o if quando ele é falso e assim por diante. O que essas ferramentas conseguem fazer hoje, elas conseguem olhar pra cada uma dos seus ifs e elas conseguem pensar da seguinte maneira, olha, qual que é o valor que eu tenho que por nesse if pra que esse if seja verdadeiro, ok? Achei esse número, vou rodar de novo. Qual que é o número que eu tenho que passar pra esse if aqui pra que esse if seja falso? Só que, poxa vida, isso é um super trabalho, como que você escreve um programa que faz isso, certo? Porque o desenvolvedor pode escrever um if de milhares de maneiras diferentes, os valores do if podem vir de várias variáveis de maneiras diferentes, como que você escreve um programa que entende o que o seu programa faz? Não é fácil, né? Então, hoje, as pesquisas mais bem-sucedidas nessa área, eles usam o que a gente chama de algoritmo de busca, né? Que é um algoritmo genético por trás dos panos, né? Então, o que a gente tenta fazer é, a gente pensa num cenário completamente aleatório, né? Então, sei lá, você quer testar o seu if ali, ele recebe que o seu if tem duas variáveis, a e b. Pensa em qualquer número pra b, certo? E aí, roda o programa. Só que, por baixo dos panos, instrumenta a JVM, e aí, você vai descobrir qual que é a comparação que tá acontecendo no if, né? Então, vamos supor que o seu if é, o a tem que ser maior que 10. Você, aleatoriamente, pensou no número 1 pro a. 1 é menor que 10? É, verdadeiro. Péssimo exemplo. Vamos supor que você, aleatoriamente, pensou no número 15. 15 é menor que 10? Não, não é. Mas 15 tá quase lá, tá só a 5 de diferença, né? O que a gente faz? A gente tenta melhorar esse número, né? Então, vai procurando uma entrada que, quando chega naquele if, a avaliação do if é quase o que você espera, entendeu? Então, é uma busca, né? A gente sabe onde a gente quer chegar, né? Então, quando eu começo a rodar um programa, eu sei que, olha, eu quero fazer com que esse if aqui seja verdadeiro. Eu saio procurando por valores que façam o if ser verdadeiro. É assim que a grande maioria das ferramentas que funcionam bem hoje funcionam. Elas instrumentam a sua JVM e elas saem procurando por valores que vão fazer você chegar no seu 100% de cobertura. Aí, obviamente, essas ferramentas são espertas também, né? Elas sabem que você escreve um monte de bug, sei lá, quando você passa um nulo pra aquela função ou quando você passa um inteiro, quando você passa um negativo, quando você passa zero. Elas sabem que, se você tem um loop, a condição de parada do loop é um lugar pra bug, certo? Então, ela sai experimentando todos esses pontos pra você de maneira automatizada. Pra Java, em particular, tem uma ferramenta muito madura hoje em dia, chama EvoSwitch. Tipo, a gente põe um link aí no podcast, eu imagino, né? Mas essa ferramenta, você fala, senhora, essa aqui é a minha classe Java. E ela sai escrevendo o teste pra você, ela vai buscar 100% de cobertura de código. Consegue sempre chegar em 100% de cobertura de código? Depende, né? Ainda tem muito desafio, né? Então, sei lá, se tem uma classe ali que fala com um banco de dados, ela não vai conseguir inserir dados no seu banco, né? Mas se a sua classe estiver bem isolada, ela consegue gerar testes, sim, com 100% de cobertura. Então, tá aí uma maneira que você, né? Pra você, como humano, não tem mais que criar teste. Você deixa a máquina criar o teste pra você. Muito maluco? Um pouquinho. É meio esquisito imaginar que alguma coisa vai escrever todos os testes, mas qual é o como é que é o uso de uma dama? Mas ô Maurício, eu penso o seguinte, você como desenvolvedor, se eu falar pra você, Maurício, escreve teste pra mim pra essa classe aqui. O que que você intuitivamente vai fazer? Você vai basicamente olhar os ifs e fors e condições ali, certo? Aí você sabe que você quer cobrir o caminho feliz do if, o caminho triste do if, você vai executar o for e assim por diante. E você, como humano, você olha aquele if, você fala, ó, aqui o if A maior do que 10, puxa, eu tenho que passar um A maior do que 10 e um A menor do que 10, certo? Você consegue ver isso. A máquina não consegue. Então é mais fácil a gente ensinar a máquina, olha, é assim que você vai fazer esse if ser verdadeiro. Vai tentando com esses números aqui até esse if ser verdadeiro. É a mesma coisa que você faz, só que a gente ensinou a máquina a fazer de uma maneira um pouquinho diferente. Eu acho incrível, especialmente pensando no cenário de manutenção de um sistema legado, né? Porque testes sempre são uma forma de documentar muito interessante, ao meu ver. Eu sinto um pouco de falta, na verdade, de trabalhar em projetos com a cobertura minimamente decente de testes, tipo, por isso. Era uma forma pra mim de entender o código sem precisar ler o código. O que fica na minha cabeça, à medida que tu explica essas ferramentas, Anish, tanto a Evelyn Suite, que é famosa mesmo, quanto as do Estado da Arte, a da qual ela faz parte, é o tempo que isso leva, né? Porque me lembra muito a situação do programa da Parada, né? Tome, dado um programa e dada essa entrada, isso vai funcionar ou não vai, né? Vai parar ou não vai. E à medida que a gente está desenvolvendo um novo sistema ou inserindo novas features, em que momento desse ciclo é que essas execuções seriam realizadas? Durante o build? Durante a pré-deploy? Porque eu não imagino como isso poderia ser eficiente o suficiente pra inserir na minha rotina de desenvolvimento, né? Tipo, fiz uma alteração no código, salvei, gera e roda todos os testes. Isso não me parece viável. Isso é uma excelente pergunta, Roberta. E acho que essa é uma outra definição que a gente tem que deixar claro, né? Porque a gente como desenvolvedor, a gente escreve testes e a gente gosta disso porque também serve como documentação, né? Igual você comentou. Essas ferramentas, elas escrevem testes pra que você depois consiga olhar pro teste de Unity e entender o que está acontecendo. O objetivo delas é pura e simplesmente achar bug pra você. Então, se você olhar pro teste que essas ferramentas escrevem, eles são ilegíveis. Um humano não consegue entender, né? Os nomes de variáveis são horríveis, o nome do método de teste não faz sentido nenhum. Geralmente é teste 1, teste 2, teste 3, né? Não faz sentido. Essas ferramentas são pra você deixar rodando no seu build de noite e no dia seguinte, se tiver ali teste que achou bug, você vai olhar, você vai entender e aí você como desenvolvedor vai escrever um teste bonitinho, elegante, que explica o caso, que é legível por um outro ser humano. Então, essas ferramentas, elas não escrevem o teste pra você pra que você mantenha isso na sua bateria de testes. Você deixa ela rodando e ela vai achar bug. E ela achou bug e você aí pega e faz o que você gosta de fazer. Corrige o bug e escreve o teste. Você comentou do tempo de execução. Esse é um problema, certo? Esses algoritmos de busca, eles são não determinísticos, né? Você vai rodar ele duas vezes, você não sabe... As duas execuções vão ser diferentes, né? E você não sabe quando que ele vai achar alguma coisa. Ele pode ficar procurando pra sempre. Então, geralmente o que a gente faz é definir um budget de tempo pra essa ferramenta. Então, você fala assim, é o switch. A gente quer que você consiga 100% de cobertura, mas você só pode rodar por 30 minutos. E ele vai por 30 minutos tentar buscar. Depois de 30 minutos, ele vai falar, eu parei aqui e eu consegui cobrir X% daquela sua classe. Ou você vai botar pra rodar de noite lá nos seus temas inteiros e você vai falar que o budget é 6 horas pros seus temas inteiros. Depois de 6 horas, ele vai parar e ele vai falar, olha, eu criei 2 milhões de testes e eu achei um bug. E eu ainda não consegui cobrir esse resto. Sem dúvida nenhuma, esse é um grande desafio. E hoje, na área de pesquisa, as pessoas têm tentado fazer muito é realmente otimizar isso, né? Por baixo dos panos, é tentar guiar o algoritmo cada vez melhor pra que ele não fique preso em lugares que, né? Que ele pode ficar preso se você entende um pouquinho de inteligência artificial, você não quer que o seu algoritmo caia no mínimo local, no máximo local, né? Você quer que ele saia explorando e que se ele ficou travado, ele continue, né? Ele mude de ideia e tente alguma outra coisa. É justamente aí que a galera tem otimizado bastante hoje. Deixa eu tentar entender um pouco mais o que que acontece nisso, Anish, porque ainda não ficou claro o quão mágico isso é. Se eu tenho um pedaço de código que testa, que lá dentro ele pega e conta quantos números de uma lista são pares. Então ele percebe, ele vai chegar a perceber que, olha, eu tô pegando a quantidade de números que são pares lá dentro. Então ele vai tentar forçar aquele if dar zero pra cair numa outra condição. Então ele vai ficar chutando arrays e listas até chegar numa que não tem números pares pra fazer aquele if dar false e falar, deixa eu testar essa outra navegação aqui na árvore de possibilidades. Exato, só que esse é o ponto, Paulo, ele não pode simplesmente chutar. Porque se ele for chutar, certo? Então ele tem que chutar e ele tem que saber o quão bom é o chute dele, certo? Então quando você tá desenhando um algoritmo desse, a coisa mais importante é escrever o que a gente chama de fitness function. É uma função, realmente uma função matemática, que você fala, olha, se o meu chute é esse, o quão perto de chegar na solução eu estou, entendeu? Por exemplo, uma fitness function fácil pra uma condição, por exemplo, A maior do que 10, certo? E você vai começar a gerar números aleatórios pra A. Se você gerar, eu falei A maior do que 10, né? Se eu gerar aleatoriamente o número 1, 1 é maior que 10? Mas e se eu puder dar uma nota pra o quão bom esse cara é, esse indivíduo é, eu vou falar 9, certo? Porque ele é 1, eu quero chegar no 10, a diferença é 9. Se eu chutar um outro número e eu falar 9, 9 é maior que 10? Não, mas puxa, tá quase lá, certo? Esse chute 9 é melhor do que o chute 1, porque o 9 tá mais perto de fazer o if ficar verdadeiro do que o 1. E se você conseguir pensar uma maneira de chegar nesse número, você consegue começar a ensinar o algoritmo a, olha, eu pensei em um monte de chute aqui, esse aqui parece o melhor, deixa eu fazer uma pequena modificação nele, porque eu tô quase lá, certo? Deixa eu fazer uma pequena modificação nele e tentar de novo. É mais ou menos isso que acontece por baixo dos panos de um algoritmo genético. E aí o que a galera do EvoSuite fez, foi olhar para cada possível operação que você faz no seu código, maior, maior, igual, menor, certo? Arrays, etc e tal, é pensar em, puxa vida, vai acontecer uma comparação aqui. Como que eu consigo dar um número pro quão bom essa variável aleatória que eu girei é? Maurício, qual nível de intervenção um desenvolvedor ou uma desenvolvedora precisa ter para que essas execuções funcionem? Um fator de reconhecimento de falha, seja, por exemplo, se o sistema jogar uma exceção, certo? Eu tô aqui gerando meus casos, executando meus casos de teste automaticamente, no meu vídeo noturno, e aí, nossa, o sistema deu crash, peguei uma exceção que não foi tratada e tal. Mas em termos de entradas e saídas esperadas, né, que não necessariamente levantam exceção, como que o EvoSuite, ou uma ferramenta do tipo, identifica que tem um bug nesse sistema? Então, eu preciso descrever essas entradas e saídas em algum lugar, já que eu não tô escrevendo o teste em si. Esse é o problema do oráculo, né? Então, por exemplo, se eu der uma entrada válida para o programa, e o programa me der uma saída inválida, a ferramenta não vai saber que essa saída foi inválida, certo? Então, a gente tem oráculo que a gente sabe que é sempre um bug. Então, se tomou um pointer exception, a ferramenta vai considerar isso um bug. Mas, por exemplo, se você deu um inteiro para a sua função, ela devolveu um inteiro, para ela deu certo, certo? Ela vai escrever um teste, esse teste vai ter a acertação ali, vai estar com o valor errado. Então, bug relacionado à regra de negócio, essas ferramentas não conseguem, porque a ferramenta não consegue saber se o resultado é certo ou não, certo? Se a implementação bate com o requisito que você tem ali. Eu só quero saber se uma palavra é palíndromo ou não. Então, se o reverso dela é ou não é igual a ela mesma. Se eu não escrevi teste nenhum, não tem como o software com inteligência artificial entender que a saída esperada para a palavra... Socorro, homem, subir no ônibus em Marrocos, o resultado é true e para a outra frase é false. Então, como que fica isso? O que me adianta? Se ele só verifica se... Vamos montar um monte de dado aleatório e eu vou forçar para ver se sai alguma exception de uma maneira inteligente. Legal, entendi que isso é possível, faz sentido. Mas como que ataca esse mais de regra de negócio, que eu acho que é onde a Roberta está caindo? Esse aí ainda não tem uma solução muito boa na academia, certo? Tem muita gente tentando aplicar um pouquinho de processamento de linguagem natural para olhar o seu requisito e tentar mais ou menos chutar o que tem que sair, mas ainda está bem longe de funcionar. Mas o ponto, Paulo, é que você diminuiu muito, o seu exemplo diminuiu muito a vantagem desse tipo de ferramenta, né? Óbvio que regra de negócio é fundamental, mas evitar esse tipo de no pointer exception ou qualquer outra exceção que pode acontecer te evita falhas de segurança, certo? Falhas de performance, que em muito sistema é fundamental, certo? No pointer exception, depende do software que você está desenvolvendo, certas vezes o usuário afinal tomar um erro 500, qual o impacto? Baixo, talvez, né? Não faz tanto sentido usar esse tipo de ferramenta, mas no caso onde você tomar um erro 500, uma no pointer exception, possa levar para um erro de segurança, um erro de performance, esse erro, você não pode não fazer esse tipo de teste. E você aí está pensando também no caso de que, poxa vida, eu vou só passar no pointer para uma classe, né? Porque a gente está discutindo tudo a nível de classe, mas pensa nisso agora no nível de sistema, eu quero testar o meu software inteiro, eu posso achar um erro 500 numa combinação bizarra de passos que o usuário tem que dar na sua aplicação web, que você como humano pode não conseguir ver. Então, apesar dessa ferramenta não conseguir testar regra de negócio, isso ainda fica um pouquinho, né? Fica com o desenvolvedor, com o ser humano, é a ferramenta que consegue automatizar o teste que você como humano não consegue ver bem, né? Que olha, poxa vida, meu software vai explodir se acontecer essa sequência de passos. E que é extremamente comum, né? Que é extremamente comum, que na verdade se você olhar em projeto open source e fuçar GitHub, etc e tal, uma pancada disso, eu não vou falar o número para não chutar aqui, mas eu acharia que uma grande parte desses números são no pointer exceptions que você vê por aí, certo? O cara reporta um pointer exception que aconteceu naquele framework, que aconteceu naquela biblioteca. Então, esse tipo de ferramenta tem muita vantagem. O Google até anunciou esses dias atrás um projeto deles que chama Fuzzing qualquer coisa, eu esqueci o nome, coloco nos links depois. Fuzzing também é uma técnica para testar software de maneira automatizada e o Google já corrigiu mais de mil e não sei quantos bugs em projetos open source de todo mundo só com esse tipo de técnica. Mas será que para esses casos mais extremos a gente não se beneficiaria muito mais usando linguagens onde não é possível fazer com que isso aconteça? Porque a gente tem várias linguagens de programação, a gente tem muita pesquisa hoje, inclusive em tipos dependentes, a gente tem Idris aí, que é uma linguagem por tipos dependentes que dá para você usar hoje, que é meio filha de Haskell lá que tem essas coisas, você define exatamente o que vai ser a entrada e é impossível você acessar um item no array que não existe. O compilador não deixa você fazer isso, o compilador proíbe. Você está trabalhando com uma linguagem como Rust que torna você ter um no pointer exception e você desalocar a memória incorretamente, que é outro problema que é muito grande que a gente vê. Seria também interessante a gente correr para esse outro lado em linguagens que tornam esse tipo de problema impossível de acontecer? Sem dúvida. Se você perguntar para mim, para mim a linguagem, quanto mais chata ela for, melhor. Mas acho que, ainda assim, a ideia de você pensar em algoritmos e inteligência artificial para testar vai fazer sentido, porque você provavelmente vai ter um oráculo que você consegue automatizar. Então, se no pointer exception não é mais um problema para você, você pode pensar em um contextual para a sua aplicação. Por exemplo, você desenvolve um e-commerce qualquer. Você sabe que não importa o que o usuário faça no sistema, o carrinho nunca pode ser com valor negativo, certo? Então, você pode ensinar uma ferramenta, olha, sai explorando o meu site aqui de todas as maneiras possíveis e se em algum momento você achar um número negativo no carrinho, reporta que é um bug. O Facebook, por exemplo, ele tem um trabalho muito legal, dá até para ver vídeo na internet, onde eles testam a aplicação mobile deles usando algoritmos de busca. E o critério de parada deles é a app crashou, a app simplesmente fechou no seu celular Android, no seu iPhone. O no pointer exception é um exemplo, mas você consegue pensar em vários outros oráculos que você pode definir. Pode ser performance, certo? Você não importa o que o usuário faça no seu sistema, ele sempre tem que responder em menos de, sei lá, x milissegundos. E aí você deixa o seu software explorar casos que ele nunca vai pensar. Nesse caso de criação de oráculos, como é que funciona isso? Para as ferramentas que a gente tem hoje, é simples, o que é que eu tenho que dar de entrada? Se eu não conhecendo nada de inteligência artificial, eu consigo fazer uma solução especializada dessas para a minha aplicação, para o meu caso pessoal? Não, ainda é um saco. Você tem que entender um pouquinho de como esses algoritmos funcionam por baixo dos panos. Desenhar uma fitness function é muito trabalhoso, mas para mim, se você me perguntar qual que é o futuro de um testador de software, para mim vai ser o cara que não só sabe fazer teste requisito, mas depois ele senta e ele escreve.